{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate\n",
    "# !pip install rouge-score\n",
    "# !pip install scikit-learn\n",
    "# !pip install streamlit\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import evaluate\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/deveshsurve/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/deveshsurve/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/deveshsurve/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load evaluation metrics\n",
    "rouge = evaluate.load('rouge')\n",
    "bleu = evaluate.load('bleu')\n",
    "meteor = evaluate.load('meteor')\n",
    "exact_match = evaluate.load('exact_match')\n",
    "f1 = evaluate.load('f1')\n",
    "# bertscore = evaluate.load('bertscore')\n",
    "# unieval = evaluate.load('unieval')\n",
    "# geval = evaluate.load('geval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import streamlit as st\n",
    "\n",
    "openai_client = OpenAI(api_key=st.secrets[\"openai\"])\n",
    "\n",
    "# OpenAI model answer function\n",
    "def get_answer_openai(question, context):\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Answer the question based on the context:\\n\\nContext: {context}\\n\\nQuestion: {question}\\nAnswer:\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Prepare the dataset\n",
    "questions = []\n",
    "reference_answers = []\n",
    "with open('eval_data.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        questions.append(data['prompt'])\n",
    "        reference_answers.append(data['referenceResponse'])\n",
    "\n",
    "# Check if GPT answers are already saved\n",
    "if os.path.exists('gpt_answers.json'):\n",
    "    with open('gpt_answers.json', 'r') as f:\n",
    "        gpt_answers = json.load(f)\n",
    "else:\n",
    "    # Generate answers using GPT\n",
    "    gpt_answers = []\n",
    "    for question in questions:\n",
    "        answer = get_answer_openai(question, context=\"\")  # You can add context if needed\n",
    "        gpt_answers.append(answer)\n",
    "\n",
    "    # Save GPT answers to a file\n",
    "    with open('gpt_answers.json', 'w') as f:\n",
    "        json.dump(gpt_answers, f)\n",
    "\n",
    "# Compare generated answers with reference answers using evaluation metrics\n",
    "rouge_scores = rouge.compute(predictions=gpt_answers, references=reference_answers)\n",
    "bleu_scores = bleu.compute(predictions=gpt_answers, references=reference_answers)\n",
    "meteor_scores = meteor.compute(predictions=gpt_answers, references=reference_answers)\n",
    "exact_match_scores = exact_match.compute(predictions=gpt_answers, references=reference_answers)\n",
    "# f1_scores = f1.compute(predictions=gpt_answers, references=reference_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qm/2m60vf_d5z3b2hf93l6yznfr0000gn/T/ipykernel_5541/2827803265.py:19: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the scores\n",
    "rouge_score = rouge_scores['rougeL']\n",
    "bleu_score = bleu_scores['bleu']\n",
    "meteor_score = meteor_scores['meteor']\n",
    "exact_match_score = exact_match_scores['exact_match']\n",
    "\n",
    "# Create a list of scores and metric names\n",
    "scores = [rouge_score, bleu_score, meteor_score, exact_match_score]\n",
    "metrics = ['ROUGE-L', 'BLEU', 'METEOR', 'Exact Match']\n",
    "\n",
    "# Create a horizontal bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(metrics, scores, color=['blue', 'orange', 'green', 'red'])\n",
    "plt.title('Comparison of Evaluation Metrics')\n",
    "plt.xlabel('Scores')\n",
    "plt.xlim(0, 1)  # Assuming the scores are between 0 and 1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Databricks solves the problem of unifying data engineering and data science on a single platform, allowing organizations to collaborate more effectively and analyze their data more efficiently.',\n",
       " 'Data engineers, data scientists, and analysts should use Databricks.',\n",
       " 'Databricks is being used for data analytics and machine learning purposes.',\n",
       " 'Yes, Databricks makes it easier for enterprises to adopt Apache Spark.',\n",
       " 'Databricks is a company founded by the creators of Apache Spark. It offers a unified analytics platform that utilizes Apache Spark as its core technology. Therefore, the relationship between Databricks and Apache Spark is that Databricks builds on and enhances the capabilities of Apache Spark for data processing and analytics.',\n",
       " 'Databricks is priced based on a subscription model, with prices varying depending on the features and capacity needed by the customer.',\n",
       " 'Yes',\n",
       " 'Yes, Databricks is open to developers to build Apache Spark applications.',\n",
       " 'You can get data into Databricks by using various means such as uploading data files directly, connecting to external data sources using JDBC/ODBC connectors, or streaming data into Databricks using tools like Apache Kafka or Apache Spark Structured Streaming.',\n",
       " 'Advanced security features such as access controls, encryption at rest and in transit, and role-based access control.',\n",
       " 'Yes, Databricks does interoperate with other Apache Spark distributions.',\n",
       " 'Yes, Databricks is available on other cloud hosting platforms such as AWS, Azure, and Google Cloud Platform.',\n",
       " 'Yes, Databricks provides isolation mechanisms when deployed in your account.',\n",
       " 'Yes, you can run Databricks in your own datacenter/cluster using Databricks Runtime for on-premises. This allows you to deploy and manage Databricks services on your own infrastructure, giving you more control and flexibility over your data processing and analytics capabilities.',\n",
       " 'Yes, Databricks does persist data in order to provide efficient data processing and analysis.',\n",
       " \"Yes, you will need to transfer your data into Databricks' AWS account in order to analyze and work with it using their platform.\",\n",
       " 'Only you will have access to your data and notebooks.',\n",
       " 'You can find out more about Databricks Security on their official website, in their documentation, or by contacting their support team.',\n",
       " \"The cloud provider typically has the right to store, manage, and protect customer data according to their terms of service and privacy policy. However, they do not have the right to access, use, or share customer data without the customer's permission or as required by law. The customer typically retains ownership of their data while using the cloud services.\",\n",
       " 'Databricks follows industry security controls and practices related to data privacy, encryption, access control, and compliance with regulations such as GDPR and HIPAA.',\n",
       " 'Yes, Databricks has a business continuity plan in place.',\n",
       " 'To find out more information about Databricks security, you can visit the Databricks website and navigate to the security section. Additionally, you can reach out to Databricks directly through their contact information on their website or inquire about security documentation from their sales or support team.',\n",
       " \"I'm sorry, but the answer is not complete. It seems like you are asking for a specific action related to Databricks careers, so the appropriate answer would be to check the Databricks website for job openings and opportunities.\"]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qnarag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
